# Final Report

### Differences From Original Design

| Feature | Original Design | Final Implementation | How It Changed |
|---------|----------------|---------------------|----------------|
| **LinkedIn Import** | OAuth/API integration or scraping | File upload (CSV/JSON) | Switched from API/scraping to user-uploaded export files for ethical compliance and LinkedIn restrictions |
| **User Flow** | Separate steps for account creation, profile setup, network creation | Unified flow with guided onboarding | Streamlined into a single guided process that creates account, profile, and network automatically |
| **Export** | Not specified | CSV export functionality | Added new feature to allow users to download their network data for backup/portability |
| **Tutorials** | Not specified | Dedicated tutorials page with step-by-step guides | Added educational content to help users understand platform features and workflows |
| **Network Management** | Manual node/edge addition | Automated additions via "Add Connection" button | Single-button flow that creates node, adds to network, and creates edge automatically |
| **Search** | Semantic search only | Semantic + standard text/filter search | Added traditional search alongside AI-powered semantic search for flexibility especially in the case of technical issues with semantic search |

## Design Summary

### Disambiguation of Mutliple Users

This is one concept that we discuseed (more a feature) we determined through meetings with our TA and amoung ourselves that it may not have been critical to the functionality and purpose of our project. Therefore it was deprioritized. Additionally the user flow was the main reason we changed the design of this function. In teh case you add 600 people the balance between allowing the user to undersatnd the matching that is going on and itntervene in the case of a problem while not bothering them too much is difficult. With intelligent matching the question is a complex one. "Are these 2 bits of data representative of the same people?" It is complex and messing it up can greatly hurt one user's experience and also the privacy of another user. Since this would allow a user to connect to and view data of a person someone else added.

In researching similar solutions we saw 2 options: Complex algorithms that are trained for the purpose of disambiguation specifically, and a simple rule based system that is accountable and easy to undedrstand. The best mix for us is a simple rule based system which lets us be confident in the disambiguation and no have to require user input to make it happen. Therefore we implemented this system fo rlinked in imports only and match based on the unique profile url. This simple rule enables the connection we discussed without complicating profiles or user flows. However this rule is not implemented between different accounts due to the privacy consideraitons. Though possible we think that the privacy considerations would be outside of the scorpe of the ethical analysis we did for the project.

### Ethics of importing
Our original plan was to use a system similar to those commercially available online, which scrape profile information on a per-profile basis (e.g., profile_id → information). However, despite these sources existing, many are vague about how they accomplish this. Upon reviewing the LinkedIn API, we discovered that a change was made around six years ago to limit the amount of information accessible through standard means. This is because LinkedIn decided to monetize access to this data, making it available only to their corporate partners—requiring negotiation of a hefty payment package. Despite these factors, it is clearly still possible to get information from LinkedIn, and since LinkedIn still permits it (for a price), the use of that data is not inherently unethical. However, after this research, it became clear that knowingly subverting the explicit changes LinkedIn has made would be unethical.

The second option I considered was using a method similar to "Sign in with LinkedIn." If you have ever signed in with Google, you may have been prompted with a question asking to share your email or contacts with the app. My hope was that, since this is a common paradigm centered around the user's own information, this would be possible with LinkedIn. However, from developer testimonials, it appears that the sign in with LinkedIn feature is primarily designed for authentication, not for streamlined integration or data import. Therefore, the information we could access would be limited.

This put me in a difficult position to design an importing system that would both meet our ethical standards and comply with the restrictions set by LinkedIn. While continuing to scrape could be possible (and many others do, even scraping and selling the data of accounts they do not own), that approach seemed inappropriate.

In searching for apps that provide value while allowing import of data they are not directly connected to, I found an example: https://www.swipestats.io. This service allows users of dating apps like Tinder to visualize their swiping and interaction history. Dating app companies do not like this use case, because it is often used by people to comment on the futility of dating apps—creating visualizations about the ratio of time spent using the app to actual dates or relationships. As a result, the app cannot integrate directly with Tinder through an API or similar mechanism. The solution they developed is to allow users to import their own data via a file upload. The app provides instructions on how to request your data from Tinder and which file to upload to quickly import everything. In fact, it does not offer any other way to add data.

Despite these limitations, the app is undoubtedly popular. You can see hundreds of people posting its visualizations in forums like https://www.reddit.com/r/Tinder/. This inspired the implementation of our import concept: we allow users to request their LinkedIn data (with instructions on how to do so), then import that data into our system—along with supporting other manual additions and data from different sources (since an LLM will review any file passed in and attempt to match it dynamically to our internal concept).

### Frontend Design

A critical aspect of an end design that I think is really important that we learned throughout this process is the question of separation of the idea of what we have in the background and what the user needs to do. Throughout the process, it seems like in design. We really didn't think through that as much as we should have, but it immediately became a parent that that's the first thing that the users cared about. They really cared about journeys To get to an end. For example, part of what they were thinking about when they looked at the front end was not can I sign in something that they were able to do successfully but in fact, they were interested in in this process of signing in how can we break this up so while it all may be using the same sign in concept it makes it clear to the user what they're doing and each place and move through ask me for my name first, then the key my picture then the key pieces of information then give me the optional pieces of information or some clues on the best way to start and then finally create my a rather than have that all available at the same time. The second thing that's really important I think is that naming and decisions about what elements and buttons are included should not be related to the back end design at all the names for what things technically are with notes and edges are not very relevant to users and well that makes it easy for us to discuss design decisions. It makes it very difficult for a user to come in and use the app who hasn't thought about it.

 Finally, I think the idea of simplifying what the user is doing is important a discussion that we had during our user testing was that apps like Instagram and other modern major pieces of software are designed to seamlessly put you on little paths while they may have a lot of extra settings and other things that you can interact with the real goal is that it will confidently place you on a path of a small set of actions to do what you wanna do if you want to post it puts you in a little carousel that takes you through the steps of posting even if you want to comment it puts you in a little set of actions where you click on the thing you wanna comment on you write down the information you want you add your tags you add your emojis then you click send if you wanna share something. You click the share button then there's maybe three or four different things that you have to do, but it's done in a small sequence so that it's almost like a little jump out. This was a lot of how they were discussing the processes that we were asking to do as tasks and they were wondering if we knew that these were tasks that were important why couldn't we design to match that? I thought this was a really good callout and it really shaped our choices in design designing the second version after we got our Feedback.

## Reflections

### Cole

I learned a lot about the importance of understanding and planning user journeys, and how crucial it is to communicate this clearly and develop a shared understanding within the group about how to approach the front end. We received the most feedback from a user and our TA regarding the user journey, especially the naming and functionality of the frontend. This led to a night and day difference in feedback before and after we discussed these issues and created new frontend components.

I also saw the benefit of having clearly defined and distinct concepts well specified at the start of implementation. As we made frontend changes, we were able to iterate and test rapidly since the backend components were distinct and well defined. Compared to my experience on the first project, I was much more able to quickly and efficiently modify both frontend and backend components after conceptualizing the main components of our backend as concepts, distinct from the frontend pages.

Additionally, I learned a lot about the ethics of implementing projects using other people's data. It is a complex area with no firm rules, and the compromises and discussions around how to do this responsibly are thought-provoking. I realized that one of the main impediments to implementing your ideas is having an innate knowledge of the moral reasoning and norms in an area and making sure what you are doing fits within them.

### Ivy

During this project, I focused on backend development and also worked on the frontend to fix bugs and implement small features. This project has been a meaningful learning experience for me, both technically and collaboratively. One of the lessons I learned was understanding how essential iteration is in any coding project. Ideas rarely stay the same and features that seemed straightforward in the beginning ended up be changed as we continued to work on the project. Some concepts had to be rethought through once we implemented more of the website and noticed missing functionalities. Feedback from user testing and mentor meetings also played a large role in shaping our final product. They showed us issues we hadn’t noticed or hadn't considered, and allowed us to create a more intuitive app.

Working as part of a team also taught me the importance of communication, task splitting, and integrating code. A large portion of my time went into debugging and trying to make different pieces of the project fit together smoothly, which reinforced the value of clean code and clear documentation. I also spent time hosting the external AI service we used, txtai. Running into issues with Render made me explore other hosting platforms, which helped me better understand the deployment process and the trade-offs of different hosting environments.

### Jing

For this project, I focused on frontend development and I had to initialize the first prototype of the frontend. Initializing the frontend requires understanding all backend concepts to be able to pull it together, and along the way, get a real feel of what is the ideal user journey we want for our users. It was really hard in the beginning to get a working frontend prototype that has all the backend concepts functioning and syncing with each other. I enjoyed testing out bugs we wouldn't realize if concepts never had to interact with each other, but the challenge is still there, and only realized the importance of shared vision on data structure among the backend concepts as I was trying to call backend functions. It was really difficult to understand which variable in this concept is equivalent to the other variable in the other concept even when sync.ts files are provided, because MongoDB ids for those data structures add on to the confusion on syncing data between collections. Overall the challenges of this project taught me what are some good practices one should use in the industry when collaborating with others on full-stack applications.

### Jenna
Working on this final project was honestly really challenging but also super rewarding. I feel like I truly learned so much (kind of forcefully) in such a short amount of time, especially during those cram-weeks before deadlines. I got really familiar with Deno, MongoDB, Vue, and especially txtai and Render. My main responsibility for this project was building the semantic search feature, and since our team picked txtai early on, I ended up becoming pretty familiar with it. But I'm glad I did because it's a very useful framework to know, even though it really tested my sanity a few times.

Deploying txtai, in particular, was a super frustrating experience to say the least. I could probably write an entire essay just on that alone, but to keep it short, it would work perfectly fine after deployment, and then randomly stop working even though on Render it still stated "deployed" and had no logs recording the shutting down of service. Even with help from Render-expert Eagon, we couldn't figure it out. In the end, I just paid for the $9 Render subscription to save both myself and my teammates from even more headache. The paid plan stops the service from going cold after inactivity, which finally made out txtai and backend setup work consistently (at least as I'm writing this...).

Overall, I really enjoyed working with this team. Everyone was super responsible, responsive, and just easy to work with. I also really liked our project idea, and I think the things we learned in this project are going to be useful far beyond the class.
